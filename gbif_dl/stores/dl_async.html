<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gbif_dl.stores.dl_async API documentation</title>
<meta name="description" content="Async based fast downloader." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gbif_dl.stores.dl_async</code></h1>
</header>
<section id="section-intro">
<p>Async based fast downloader.</p>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/plantnet/gbif-dl/blob/5c00725e4e433dca28393b7f6f9b4058b8c6437e/gbif_dl/stores/dl_async.py#L1-L328" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Async based fast downloader.
&#34;&#34;&#34;
import asyncio
import inspect
from pathlib import Path
from typing import AsyncGenerator, Callable, Generator, Union, Optional
import sys
import json
import hashlib
import random
import logging
from tqdm.contrib.logging import logging_redirect_tqdm

if sys.version_info &gt;= (3, 8):
    from typing import TypedDict  # pylint: disable=no-name-in-module
else:
    from typing_extensions import TypedDict

from collections.abc import Iterable


import filetype
import aiofiles
import aiohttp
import aiostream
from aiohttp_retry import RetryClient, ExponentialRetry
from tqdm.asyncio import tqdm, tqdm_asyncio
from ..utils import run_async
from . import MediaData


class DownloadParams(TypedDict):
    root: str
    overwrite: bool
    is_valid_file: Optional[Callable[[bytes], bool]]
    proxy: Optional[str]
    random_subsets: Optional[dict]


async def download_single(
    item: Union[MediaData, str], session: RetryClient, params: DownloadParams
):
    &#34;&#34;&#34;Async function to download single url to disk

    Args:
        item (Dict or str): item dict or url.
        session (RetryClient): aiohttp session.
        params (DownloadParams): Download parameter dict
    &#34;&#34;&#34;
    if isinstance(item, dict):
        url = item.get(&#34;url&#34;)
        basename = item.get(&#34;basename&#34;)
        label = item.get(&#34;label&#34;)
        subset = item.get(&#34;subset&#34;)
    else:
        url = item
        label, basename, subset = None, None, None

    if subset is None and params[&#34;random_subsets&#34;] is not None:
        subset_choices = list(params[&#34;random_subsets&#34;].keys())
        p = list(params[&#34;random_subsets&#34;].values())
        subset = random.choices(subset_choices, weights=p, k=1)[0]

    label_path = Path(params[&#34;root&#34;])

    if subset is not None:
        label_path /= Path(subset)

    # create subfolder when label is a single str
    if isinstance(label, str):
        # append label path
        label_path /= Path(label)

    label_path.mkdir(parents=True, exist_ok=True)

    if basename is None:
        # hash the url
        basename = hashlib.sha1(url.encode(&#34;utf-8&#34;)).hexdigest()

    check_files_with_same_basename = label_path.glob(basename + &#34;*&#34;)
    if list(check_files_with_same_basename) and not params[&#34;overwrite&#34;]:
        # do not overwrite, skips based on base path
        return False

    async with session.get(url, proxy=params[&#34;proxy&#34;]) as res:
        content = await res.read()

    # guess mimetype and suffix from content
    kind = filetype.guess(content)
    if kind is None:
        return False
    else:
        suffix = &#34;.&#34; + kind.extension
        mime = kind.mime

    # Check everything went well
    if res.status != 200:
        raise aiohttp.ClientResponseError

    if params[&#34;is_valid_file&#34;] is not None:
        if not params[&#34;is_valid_file&#34;](content):
            print(f&#34;File check failed&#34;)
            return False

    file_base_path = label_path / basename
    file_path = file_base_path.with_suffix(suffix)
    async with aiofiles.open(file_path, &#34;+wb&#34;) as f:
        await f.write(content)

    if isinstance(label, dict):
        json_path = (label_path / item[&#34;basename&#34;]).with_suffix(&#34;.json&#34;)
        async with aiofiles.open(json_path, mode=&#34;+w&#34;) as fp:
            await fp.write(json.dumps(label))

    return True


async def _download_queue(
    queue: asyncio.Queue,
    session: RetryClient,
    stats: dict,
    params: DownloadParams,
    progressbar: tqdm_asyncio = None,
    logger: logging.Logger = None,
):
    &#34;&#34;&#34;Consumes items from download queue

    Args:
        queue (asyncio.Queue): Queue of items
        session (RetryClient): RetryClient aiohttp session object
        params (DownloadParams): Download parameter dict
        logger (logging.Logger): Logger object
    &#34;&#34;&#34;
    while True:
        batch = await queue.get()
        for sample in batch:
            failed = False
            try:
                success = await download_single(sample, session, params)
            except Exception as e:
                with logging_redirect_tqdm(loggers=[logger]):
                    logger.error(e.request_info.url, extra={&#34;status&#34;: e.status})
                    failed = True

            if failed:
                stats[&#34;failed&#34;] += 1
            elif not success:
                stats[&#34;skipped&#34;] += 1
            else:
                stats[&#34;success&#34;] += 1

            progressbar.set_postfix(stats=stats, refresh=True)
            progressbar.update(1)

        queue.task_done()


async def _download_from_asyncgen(
    items: AsyncGenerator,
    params: DownloadParams,
    tcp_connections: int = 64,
    nb_workers: int = 64,
    batch_size: int = 16,
    retries: int = 1,
    logger: logging.Logger = None,
):
    &#34;&#34;&#34;Asynchronous downloader that takes an interable and downloads it

    Args:
        items (Union[Generator, AsyncGenerator]): (async/sync) generator that yiels a standardized dict of urls
        params (DownloadParams): Download parameter dict
        tcp_connections (int, optional): Maximum number of concurrent TCP connections. Defaults to 128.
        nb_workers (int, optional): Maximum number of workers. Defaults to 64.
        batch_size (int, optional): Maximum queue batch size. Defaults to 16.
        retries (int, optional): Maximum number of attempts. Defaults to 1.
        logger (logging.Logger, optional): Logger object. Defaults to None.
    Raises:
        NotImplementedError: If generator turns out to be invalid.
    &#34;&#34;&#34;

    queue = asyncio.Queue(nb_workers)
    progressbar = tqdm(
        smoothing=0, unit=&#34; Downloads&#34;, disable=logger.getEffectiveLevel() &gt; logging.INFO
    )
    stats = {&#34;failed&#34;: 0, &#34;skipped&#34;: 0, &#34;success&#34;: 0}

    retry_options = ExponentialRetry(attempts=retries)

    async with RetryClient(
        connector=aiohttp.TCPConnector(limit=tcp_connections),
        raise_for_status=True,
        retry_options=retry_options,
        trust_env=True,
    ) as session:

        loop = asyncio.get_event_loop()
        workers = [
            loop.create_task(
                _download_queue(
                    queue, session, stats, params=params, progressbar=progressbar, logger=logger
                )
            )
            for _ in range(nb_workers)
        ]

        # get chunks from async generator and add to async queue
        async with aiostream.stream.chunks(items, batch_size).stream() as chnk:
            async for batch in chnk:
                await queue.put(batch)

        await queue.join()

    for w in workers:
        w.cancel()

    return stats


def download(
    items: Union[Generator, AsyncGenerator, Iterable, Path],
    root: str = &#34;data&#34;,
    tcp_connections: int = 128,
    nb_workers: int = 128,
    batch_size: int = 16,
    retries: int = 1,
    loglevel: str = &#34;INFO&#34;,
    error_log_path: Path = None,
    overwrite: bool = False,
    is_valid_file: Optional[Callable[[bytes], bool]] = None,
    proxy: Optional[str] = None,
    random_subsets: Optional[dict] = None,
):
    &#34;&#34;&#34;Core download function that takes an interable (sync or async)

    Args:
        items (Union[Generator, AsyncGenerator, Iterable, Path]): (async/sync) generator
            list or path to text file that includes urls and optional labels.
            The text file should have one url per line and optional data after a whitespace.
        root (str, optional): Root path of downloads. Defaults to &#34;data&#34;.
        tcp_connections (int, optional): Maximum number of concurrent TCP connections. Defaults to 128.
        nb_workers (int, optional): Maximum number of workers. Defaults to 128.
        batch_size (int, optional): Maximum queue batch size. Defaults to 8.
        retries (int, optional): Maximum number of attempts. Defaults to 1, which means one try.
        loglevel (str, optional): Set logger logging level.
            This shows failed downloads and a progressbar. Setting it to `ERROR` disables the progressbar.
            Setting it to `CRITICAL` disables all logging. Defaults to `INFO`.
        error_log_path (Path, optional): Writes errors to file. Defaults to None.
        overwrite (bool): overwrite files with existing `baseline` signature, Defaults to False.
        is_valid_file (optional): A function that takes bytes
            and checks if the bytes originate from a valid file
            (used to check of corrupt files). Defaults to None.
            overwrite existing files, Defaults to False.
        proxy (str): Proxy server url. Authentication credentials can be passed in URL. e.g
            `proxy=&#34;http://user:pass@some.proxy.com&#34;`. Proxy can also be used globally using environmental variables.
            See https://www.gnu.org/software/inetutils/manual/html_node/The-_002enetrc-file.html.
        random_subsets (dict[str, float]): add random subset given as a dict of class names and it&#39;s propability.
            e.g. `{&#39;train&#39;: 0.9, test&#39;: 0.1}` will result in 90% of the items
            go into a `train` subfolder and 10% into a `test` subfolder.
            The propabilities have to sum up to `1.0` to avoid an error.

    Returns:
        dict: A dict of download statistics.

    Raises:
        NotImplementedError: If generator turns out to be invalid.
    &#34;&#34;&#34;

    if isinstance(items, (Path, str)):
        if Path(items).exists():
            # ignore all string after first space
            items = [l.split(&#34; &#34;)[0] for l in Path(items).read_text().splitlines()]

    # check if the generator is async
    if not inspect.isasyncgen(items):
        # if its not, apply hack to make it async
        if inspect.isgenerator(items) or isinstance(items, Iterable):
            items = aiostream.stream.iterate(items)
        else:
            raise NotImplementedError(&#34;Provided iteratable could not be converted&#34;)

    if random_subsets is not None:
        p = random_subsets.values()
        if sum(p) != 1.0:
            raise RuntimeError(&#34;Make sure that weight probabilities add up to one&#34;)

    logger = logging.getLogger(&#34;error_urls&#34;)

    # set log format suitable for error logs and io
    formatter = logging.Formatter(&#34;%(message)s %(status)s&#34;)
    # set default log level to only receive errors
    logger.setLevel(loglevel)

    handlers = []
    if logger.getEffectiveLevel() &lt;= logging.ERROR:
        # write errors to std.out
        sh = logging.StreamHandler(sys.stdout)
        sh.setFormatter(formatter)
        handlers.append(sh)
        # in case an error path is set, also write errors to file
        if isinstance(error_log_path, str):
            fh = logging.FileHandler(error_log_path)
            fh.setFormatter(formatter)
            handlers.append(fh)

    for handler in handlers:
        logger.addHandler(handler)

    logger.propagate = False

    params = {
        &#34;root&#34;: root,
        &#34;overwrite&#34;: overwrite,
        &#34;is_valid_file&#34;: is_valid_file,
        &#34;proxy&#34;: proxy,
        &#34;random_subsets&#34;: random_subsets,
    }

    return run_async(
        _download_from_asyncgen,
        items,
        tcp_connections=tcp_connections,
        nb_workers=nb_workers,
        batch_size=batch_size,
        retries=retries,
        logger=logger,
        params=params,
    )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="gbif_dl.stores.dl_async.download"><code class="name flex">
<span>def <span class="ident">download</span></span>(<span>items: Union[Generator[+T_co, -T_contra, +V_co], AsyncGenerator[+T_co, -T_contra], collections.abc.Iterable, pathlib.Path], root: str = 'data', tcp_connections: int = 128, nb_workers: int = 128, batch_size: int = 16, retries: int = 1, loglevel: str = 'INFO', error_log_path: pathlib.Path = None, overwrite: bool = False, is_valid_file: Optional[Callable[[bytes], bool]] = None, proxy: Optional[str] = None, random_subsets: Optional[dict] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Core download function that takes an interable (sync or async)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>items</code></strong> :&ensp;<code>Union[Generator, AsyncGenerator, Iterable, Path]</code></dt>
<dd>(async/sync) generator
list or path to text file that includes urls and optional labels.
The text file should have one url per line and optional data after a whitespace.</dd>
<dt><strong><code>root</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Root path of downloads. Defaults to "data".</dd>
<dt><strong><code>tcp_connections</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of concurrent TCP connections. Defaults to 128.</dd>
<dt><strong><code>nb_workers</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of workers. Defaults to 128.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum queue batch size. Defaults to 8.</dd>
<dt><strong><code>retries</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of attempts. Defaults to 1, which means one try.</dd>
<dt><strong><code>loglevel</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Set logger logging level.
This shows failed downloads and a progressbar. Setting it to <code>ERROR</code> disables the progressbar.
Setting it to <code>CRITICAL</code> disables all logging. Defaults to <code>INFO</code>.</dd>
<dt><strong><code>error_log_path</code></strong> :&ensp;<code>Path</code>, optional</dt>
<dd>Writes errors to file. Defaults to None.</dd>
<dt><strong><code>overwrite</code></strong> :&ensp;<code>bool</code></dt>
<dd>overwrite files with existing <code>baseline</code> signature, Defaults to False.</dd>
<dt><strong><code>is_valid_file</code></strong> :&ensp;<code>optional</code></dt>
<dd>A function that takes bytes
and checks if the bytes originate from a valid file
(used to check of corrupt files). Defaults to None.
overwrite existing files, Defaults to False.</dd>
<dt><strong><code>proxy</code></strong> :&ensp;<code>str</code></dt>
<dd>Proxy server url. Authentication credentials can be passed in URL. e.g
<code>proxy="http://user:pass@some.proxy.com"</code>. Proxy can also be used globally using environmental variables.
See <a href="https://www.gnu.org/software/inetutils/manual/html_node/The-_002enetrc-file.html.">https://www.gnu.org/software/inetutils/manual/html_node/The-_002enetrc-file.html.</a></dd>
<dt><strong><code>random_subsets</code></strong> :&ensp;<code>dict[str, float]</code></dt>
<dd>add random subset given as a dict of class names and it's propability.
e.g. <code>{'train': 0.9, test': 0.1}</code> will result in 90% of the items
go into a <code>train</code> subfolder and 10% into a <code>test</code> subfolder.
The propabilities have to sum up to <code>1.0</code> to avoid an error.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>A dict of download statistics.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>If generator turns out to be invalid.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/plantnet/gbif-dl/blob/5c00725e4e433dca28393b7f6f9b4058b8c6437e/gbif_dl/stores/dl_async.py#L220-L328" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def download(
    items: Union[Generator, AsyncGenerator, Iterable, Path],
    root: str = &#34;data&#34;,
    tcp_connections: int = 128,
    nb_workers: int = 128,
    batch_size: int = 16,
    retries: int = 1,
    loglevel: str = &#34;INFO&#34;,
    error_log_path: Path = None,
    overwrite: bool = False,
    is_valid_file: Optional[Callable[[bytes], bool]] = None,
    proxy: Optional[str] = None,
    random_subsets: Optional[dict] = None,
):
    &#34;&#34;&#34;Core download function that takes an interable (sync or async)

    Args:
        items (Union[Generator, AsyncGenerator, Iterable, Path]): (async/sync) generator
            list or path to text file that includes urls and optional labels.
            The text file should have one url per line and optional data after a whitespace.
        root (str, optional): Root path of downloads. Defaults to &#34;data&#34;.
        tcp_connections (int, optional): Maximum number of concurrent TCP connections. Defaults to 128.
        nb_workers (int, optional): Maximum number of workers. Defaults to 128.
        batch_size (int, optional): Maximum queue batch size. Defaults to 8.
        retries (int, optional): Maximum number of attempts. Defaults to 1, which means one try.
        loglevel (str, optional): Set logger logging level.
            This shows failed downloads and a progressbar. Setting it to `ERROR` disables the progressbar.
            Setting it to `CRITICAL` disables all logging. Defaults to `INFO`.
        error_log_path (Path, optional): Writes errors to file. Defaults to None.
        overwrite (bool): overwrite files with existing `baseline` signature, Defaults to False.
        is_valid_file (optional): A function that takes bytes
            and checks if the bytes originate from a valid file
            (used to check of corrupt files). Defaults to None.
            overwrite existing files, Defaults to False.
        proxy (str): Proxy server url. Authentication credentials can be passed in URL. e.g
            `proxy=&#34;http://user:pass@some.proxy.com&#34;`. Proxy can also be used globally using environmental variables.
            See https://www.gnu.org/software/inetutils/manual/html_node/The-_002enetrc-file.html.
        random_subsets (dict[str, float]): add random subset given as a dict of class names and it&#39;s propability.
            e.g. `{&#39;train&#39;: 0.9, test&#39;: 0.1}` will result in 90% of the items
            go into a `train` subfolder and 10% into a `test` subfolder.
            The propabilities have to sum up to `1.0` to avoid an error.

    Returns:
        dict: A dict of download statistics.

    Raises:
        NotImplementedError: If generator turns out to be invalid.
    &#34;&#34;&#34;

    if isinstance(items, (Path, str)):
        if Path(items).exists():
            # ignore all string after first space
            items = [l.split(&#34; &#34;)[0] for l in Path(items).read_text().splitlines()]

    # check if the generator is async
    if not inspect.isasyncgen(items):
        # if its not, apply hack to make it async
        if inspect.isgenerator(items) or isinstance(items, Iterable):
            items = aiostream.stream.iterate(items)
        else:
            raise NotImplementedError(&#34;Provided iteratable could not be converted&#34;)

    if random_subsets is not None:
        p = random_subsets.values()
        if sum(p) != 1.0:
            raise RuntimeError(&#34;Make sure that weight probabilities add up to one&#34;)

    logger = logging.getLogger(&#34;error_urls&#34;)

    # set log format suitable for error logs and io
    formatter = logging.Formatter(&#34;%(message)s %(status)s&#34;)
    # set default log level to only receive errors
    logger.setLevel(loglevel)

    handlers = []
    if logger.getEffectiveLevel() &lt;= logging.ERROR:
        # write errors to std.out
        sh = logging.StreamHandler(sys.stdout)
        sh.setFormatter(formatter)
        handlers.append(sh)
        # in case an error path is set, also write errors to file
        if isinstance(error_log_path, str):
            fh = logging.FileHandler(error_log_path)
            fh.setFormatter(formatter)
            handlers.append(fh)

    for handler in handlers:
        logger.addHandler(handler)

    logger.propagate = False

    params = {
        &#34;root&#34;: root,
        &#34;overwrite&#34;: overwrite,
        &#34;is_valid_file&#34;: is_valid_file,
        &#34;proxy&#34;: proxy,
        &#34;random_subsets&#34;: random_subsets,
    }

    return run_async(
        _download_from_asyncgen,
        items,
        tcp_connections=tcp_connections,
        nb_workers=nb_workers,
        batch_size=batch_size,
        retries=retries,
        logger=logger,
        params=params,
    )</code></pre>
</details>
</dd>
<dt id="gbif_dl.stores.dl_async.download_single"><code class="name flex">
<span>async def <span class="ident">download_single</span></span>(<span>item: Union[<a title="gbif_dl.stores.MediaData" href="index.html#gbif_dl.stores.MediaData">MediaData</a>, str], session: aiohttp_retry.client.RetryClient, params: <a title="gbif_dl.stores.dl_async.DownloadParams" href="#gbif_dl.stores.dl_async.DownloadParams">DownloadParams</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Async function to download single url to disk</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>item</code></strong> :&ensp;<code>Dict</code> or <code>str</code></dt>
<dd>item dict or url.</dd>
<dt><strong><code>session</code></strong> :&ensp;<code>RetryClient</code></dt>
<dd>aiohttp session.</dd>
<dt><strong><code>params</code></strong> :&ensp;<code><a title="gbif_dl.stores.dl_async.DownloadParams" href="#gbif_dl.stores.dl_async.DownloadParams">DownloadParams</a></code></dt>
<dd>Download parameter dict</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/plantnet/gbif-dl/blob/5c00725e4e433dca28393b7f6f9b4058b8c6437e/gbif_dl/stores/dl_async.py#L41-L116" class="git-link">Browse git</a>
</summary>
<pre><code class="python">async def download_single(
    item: Union[MediaData, str], session: RetryClient, params: DownloadParams
):
    &#34;&#34;&#34;Async function to download single url to disk

    Args:
        item (Dict or str): item dict or url.
        session (RetryClient): aiohttp session.
        params (DownloadParams): Download parameter dict
    &#34;&#34;&#34;
    if isinstance(item, dict):
        url = item.get(&#34;url&#34;)
        basename = item.get(&#34;basename&#34;)
        label = item.get(&#34;label&#34;)
        subset = item.get(&#34;subset&#34;)
    else:
        url = item
        label, basename, subset = None, None, None

    if subset is None and params[&#34;random_subsets&#34;] is not None:
        subset_choices = list(params[&#34;random_subsets&#34;].keys())
        p = list(params[&#34;random_subsets&#34;].values())
        subset = random.choices(subset_choices, weights=p, k=1)[0]

    label_path = Path(params[&#34;root&#34;])

    if subset is not None:
        label_path /= Path(subset)

    # create subfolder when label is a single str
    if isinstance(label, str):
        # append label path
        label_path /= Path(label)

    label_path.mkdir(parents=True, exist_ok=True)

    if basename is None:
        # hash the url
        basename = hashlib.sha1(url.encode(&#34;utf-8&#34;)).hexdigest()

    check_files_with_same_basename = label_path.glob(basename + &#34;*&#34;)
    if list(check_files_with_same_basename) and not params[&#34;overwrite&#34;]:
        # do not overwrite, skips based on base path
        return False

    async with session.get(url, proxy=params[&#34;proxy&#34;]) as res:
        content = await res.read()

    # guess mimetype and suffix from content
    kind = filetype.guess(content)
    if kind is None:
        return False
    else:
        suffix = &#34;.&#34; + kind.extension
        mime = kind.mime

    # Check everything went well
    if res.status != 200:
        raise aiohttp.ClientResponseError

    if params[&#34;is_valid_file&#34;] is not None:
        if not params[&#34;is_valid_file&#34;](content):
            print(f&#34;File check failed&#34;)
            return False

    file_base_path = label_path / basename
    file_path = file_base_path.with_suffix(suffix)
    async with aiofiles.open(file_path, &#34;+wb&#34;) as f:
        await f.write(content)

    if isinstance(label, dict):
        json_path = (label_path / item[&#34;basename&#34;]).with_suffix(&#34;.json&#34;)
        async with aiofiles.open(json_path, mode=&#34;+w&#34;) as fp:
            await fp.write(json.dumps(label))

    return True</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gbif_dl.stores.dl_async.DownloadParams"><code class="flex name class">
<span>class <span class="ident">DownloadParams</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/plantnet/gbif-dl/blob/5c00725e4e433dca28393b7f6f9b4058b8c6437e/gbif_dl/stores/dl_async.py#L33-L38" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class DownloadParams(TypedDict):
    root: str
    overwrite: bool
    is_valid_file: Optional[Callable[[bytes], bool]]
    proxy: Optional[str]
    random_subsets: Optional[dict]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="gbif_dl.stores.dl_async.DownloadParams.is_valid_file"><code class="name">var <span class="ident">is_valid_file</span> : Optional[Callable[[bytes], bool]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gbif_dl.stores.dl_async.DownloadParams.overwrite"><code class="name">var <span class="ident">overwrite</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gbif_dl.stores.dl_async.DownloadParams.proxy"><code class="name">var <span class="ident">proxy</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gbif_dl.stores.dl_async.DownloadParams.random_subsets"><code class="name">var <span class="ident">random_subsets</span> : Optional[dict]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gbif_dl.stores.dl_async.DownloadParams.root"><code class="name">var <span class="ident">root</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gbif_dl.stores" href="index.html">gbif_dl.stores</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="gbif_dl.stores.dl_async.download" href="#gbif_dl.stores.dl_async.download">download</a></code></li>
<li><code><a title="gbif_dl.stores.dl_async.download_single" href="#gbif_dl.stores.dl_async.download_single">download_single</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gbif_dl.stores.dl_async.DownloadParams" href="#gbif_dl.stores.dl_async.DownloadParams">DownloadParams</a></code></h4>
<ul class="">
<li><code><a title="gbif_dl.stores.dl_async.DownloadParams.is_valid_file" href="#gbif_dl.stores.dl_async.DownloadParams.is_valid_file">is_valid_file</a></code></li>
<li><code><a title="gbif_dl.stores.dl_async.DownloadParams.overwrite" href="#gbif_dl.stores.dl_async.DownloadParams.overwrite">overwrite</a></code></li>
<li><code><a title="gbif_dl.stores.dl_async.DownloadParams.proxy" href="#gbif_dl.stores.dl_async.DownloadParams.proxy">proxy</a></code></li>
<li><code><a title="gbif_dl.stores.dl_async.DownloadParams.random_subsets" href="#gbif_dl.stores.dl_async.DownloadParams.random_subsets">random_subsets</a></code></li>
<li><code><a title="gbif_dl.stores.dl_async.DownloadParams.root" href="#gbif_dl.stores.dl_async.DownloadParams.root">root</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>